{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**DEVOIR 4 - Classification de textes**"
      ],
      "metadata": {
        "id": "Pbcc9QS1tnBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Numéro du groupe: \\\\\n",
        "   Noms des membres: \\\\\n",
        "   Numéros d'étudiants des membres: \\\\\n",
        "   Titre: \\\\"
      ],
      "metadata": {
        "id": "f2Eeke4Z_EkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Datasets dérivés**"
      ],
      "metadata": {
        "id": "RMUnCICdyBbs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-so3pwbPKTDX"
      },
      "source": [
        "Ce notebook est un point de départ pour le devoir 4. Dans ce devoir, vous effectuerez une étude empirique de classification. Ce notebook vous aidera à créer des ensembles de données dérivés dans la section 2 du devoir."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's start by installing spaCy\n",
        "!pip install spacy"
      ],
      "metadata": {
        "id": "bhFvS3q7Lu0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCWgl6PLKTDY"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vous avez reçu une liste de datasets dans la description du devoir. Choisissez l'un des datasets, fournissez le lien ci-dessous et lisez ce dataset à l'aide de pandas. Vous devez fournir un lien vers votre propre répertoire Github même si vous utilisez une version réduite d'un ensemble de données du répertoire de votre TA."
      ],
      "metadata": {
        "id": "EX9WQWGSwU2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vous devrez ajouter une description de l'ensemble de données et votre justification des choix effectués pour obtenir les ensembles de données dérivés."
      ],
      "metadata": {
        "id": "CSyg0jjC1jJa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Xx4qMCLKTDb"
      },
      "outputs": [],
      "source": [
        "#Load the dataset you chose.\n",
        "# Make sure the Notebook can load your dataset, just like previous assignments.\n",
        "\n",
        "# url = 'https://raw.githubusercontent.com/baharin/CSI4106-Assignment4-Datasets/main/reduced_file_cnnnews.csv'\n",
        "# url = 'https://raw.githubusercontent.com/baharin/CSI4106-Assignment4-Datasets/main/reduced_drugsComTest_raw_fiveclasses.csv'\n",
        "url = 'https://raw.githubusercontent.com/baharin/CSI4106-Assignment4-Datasets/main/reduced_file_AirPassengerReviews.csv'\n",
        "\n",
        "#provide the link to the raw version of dataset. You *need* to provide a link to *your own* github repository. DO NOT use the link that is provided as an example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(url)\n",
        "data = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "wg24OUV81Xgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ5nSY1HKTDd"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3pycllPKTDd"
      },
      "source": [
        "Ici vous créez votre pipeline TAL. load() téléchargera le bon modèle pour l'analyse (English)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQtSi8XuKTDe"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccqFArDyKTDf"
      },
      "source": [
        "L'application du pipeline à chaque phrase crée un document dans lequel chaque mot est un objet Token.\n",
        "\n",
        "Doc: https://spacy.io/api/doc\n",
        "\n",
        "Token: https://spacy.io/api/token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcaeMUL2KTDg"
      },
      "outputs": [],
      "source": [
        "#Apply nlp pipeline to the column that has your sentences (the text that will serve as input features).\n",
        "data['tokenized'] = data['??'].apply(nlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7i6ai1I8KTDg"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHRfZ2uEKTDh"
      },
      "source": [
        "Un token a plusieurs attributs, tel part-of-speech (pos_), lemma (lemma_), etc. Regardez la documentation pour voir l'ensemble des attributs.\n",
        "\n",
        "La fonction suivante est un exemple de la façon dont vous pouvez récupérer les parties du discours (POS)à partir d'une phrase. Nous renvoyons la lemmatisation car nous voulons uniquement le mot à l'infinitif."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create empty dataframes that will store your derived datasets\n",
        "\n",
        "derived_dataset1 = pd.DataFrame(columns = ['Class', 'pos'])\n",
        "derived_dataset2 = pd.DataFrame(columns = ['Class', 'pos-np'])"
      ],
      "metadata": {
        "id": "qw0a_2ySyUo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yeak1tAOKTDi"
      },
      "outputs": [],
      "source": [
        "def get_pos(sentence, wanted_pos): #wanted_pos refers to the desired pos tagging\n",
        "    verbs = []\n",
        "    for token in sentence:\n",
        "        if token.pos_ in wanted_pos:\n",
        "            verbs.append(token.lemma_) # lemma returns a number. lemma_ return a string\n",
        "    return ' '.join(verbs) # return value is as a string and not a list for countVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "147NRzwKKTDj"
      },
      "outputs": [],
      "source": [
        "#As an example, we use the above function to fetch all the verbs. We store this information in our first derived dataset\n",
        "derived_dataset1['pos'] = data['tokenized'].apply(lambda sent : get_pos(sent, ['VERB']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_bUg_fVKTDk"
      },
      "outputs": [],
      "source": [
        "derived_dataset1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuGv-NnfKTDj"
      },
      "outputs": [],
      "source": [
        "#Change this line to fetch your desired pos taggings for the second derived dataset\n",
        "derived_dataset2['pos-np'] = data['tokenized'].apply(??)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For Derived Dataset 2, you also need to include Named Entities\n",
        "#Below is just an example of obtaining such entities on a specific sentence, but you would do NER\n",
        "#on the dataset of your choice.\n",
        "#You can choose the types of entities (dates, organization, people) that you want,\n",
        "#and then in your derived dataset, just make sure you include these entities separated by spaces (as shown for verbs)\n",
        "#in a previous cell.\n",
        "\n",
        "sentence = \"apple is looking at buying U.K. startup for $1 billion\"\n",
        "doc = nlp(sentence)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "id": "NR7AdW0MfXO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pX4RgKhKTDk"
      },
      "source": [
        "Maintenant que vous disposez de vos ensembles de données dérivés, vous pouvez effectuer votre tâche de classification."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification avec MLP et Régression logistique**"
      ],
      "metadata": {
        "id": "1GhniwHtzfQt"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}